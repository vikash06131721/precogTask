# precogTask

Classical ML Models For Sentence Similarity : classical_models_sentence.ipynb

Classical ML Models For Phrase Similarity : classical_models_phrase.ipynb

4 Transformer Based Models With Cross Encoder Architecture For Phrase Similarity: cross_encoder_phrase_sim_all_models.ipynb

Roberta Base Model Trained For 30 epochs For Phrase Similarity :cross_encoder_phrase_sim_a_Roberta_Base.ipynb


4 Transformer Based Models With Cross Encoder Architecture For Sentence Similarity: sentence_similarity_all_models.py

Roberta Base Model Trained For 15 epochs For Phrase Similarity: roberta_sent_sim.py


LLAMA LLM embeddings analysis for phrase similarity: embeddings-analysis-using-a-LLM-Llama3.2-1B-phrase.ipynb

Gemma:2B LLM embeddings analysis for phrase similarity: plotGemma.py

Gemma:2B LLM embeddings analysis for sentece similarity: plotGemmaSent.py

LLAMA LLM embeddings analysis for sentece similarity: plotLlamaSent.py


###Note

Extracting the embeddings was done via Ollama deployed on a Linux server using GCP(Tesla T4 GPUs), so it might not be accessible. Other than that all the other codes can be run via jupyter notebooks.




