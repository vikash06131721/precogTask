# precogTask

- **Classical ML Models for Sentence Similarity:** [classical_models_sentence.ipynb](classical_models_sentence.ipynb)
- **Classical ML Models for Phrase Similarity:** [classical_models_phrase.ipynb](classical_models_phrase.ipynb)

- **Transformer-Based Models with Cross-Encoder Architecture for Phrase Similarity:** [cross_encoder_phrase_sim_all_models.ipynb](cross_encoder_phrase_sim_all_models.ipynb)
- **RoBERTa Base Model Trained for 30 Epochs for Phrase Similarity:** [cross_encoder_phrase_sim_a_Roberta_Base.ipynb](cross_encoder_phrase_sim_a_Roberta_Base.ipynb)

- **Transformer-Based Models with Cross-Encoder Architecture for Sentence Similarity:** [sentence_similarity_all_models.py](sentence_similarity_all_models.py)
- **RoBERTa Base Model Trained for 15 Epochs for Sentence Similarity:** [roberta_sent_sim.py](roberta_sent_sim.py)

- **LLAMA LLM Embeddings Analysis for Phrase Similarity:** [embeddings-analysis-using-a-LLM-Llama3.2-1B-phrase.ipynb](embeddings-analysis-using-a-LLM-Llama3.2-1B-phrase.ipynb)
- **Gemma:2B LLM Embeddings Analysis for Phrase Similarity:** [plotGemma.py](plotGemma.py)
- **Gemma:2B LLM Embeddings Analysis for Sentence Similarity:** [plotGemmaSent.py](plotGemmaSent.py)
- **LLAMA LLM Embeddings Analysis for Sentence Similarity:** [plotLlamaSent.py](plotLlamaSent.py)

## Note
Embedding extraction was performed using Ollama, deployed on a Linux server on GCP with Tesla T4 GPUs, and may not be accessible without similar setup. All other scripts can be run in Jupyter notebooks.
Requirements are included in requirements.txt
